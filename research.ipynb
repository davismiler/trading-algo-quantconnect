{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import serialize_keras_object\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training period\n",
    "start = datetime(2020, 1, 1)\n",
    "end = datetime(2022, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores all the minute data for our symbol from our training period\n",
    "qb = QuantBook()\n",
    "symbol = qb.add_cfd(\"XAUUSD\", Resolution.MINUTE).symbol\n",
    "history = qb.history(symbol, start, end).loc[symbol]\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records the percentage change from the previous minute instead of the value\n",
    "daily_pct_change = history[[\"open\",\"high\",\"low\",\"close\"]].pct_change().dropna()\n",
    "df = daily_pct_change\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates features and labels for the model to be trained on\n",
    "# Model will consider data from the past half hour to make a decision\n",
    "n_steps = 30\n",
    "features = []\n",
    "labels = []\n",
    "for i in range(len(df)-n_steps):\n",
    "    input_data = df.iloc[i:i+n_steps].values\n",
    "    features.append(input_data)\n",
    "    \n",
    "    if df['close'].iloc[i+n_steps] >= 0:\n",
    "        # Price went up\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels into training, validation, and test sets (60/20/20)\n",
    "# This provides better model evaluation and prevents overfitting\n",
    "train_length = int(len(features) * 0.6)\n",
    "val_length = int(len(features) * 0.2)\n",
    "\n",
    "X_train = features[:train_length]\n",
    "X_val = features[train_length:train_length + val_length]\n",
    "X_test = features[train_length + val_length:]\n",
    "\n",
    "y_train = labels[:train_length]\n",
    "y_val = labels[train_length:train_length + val_length]\n",
    "y_test = labels[train_length + val_length:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in training set (should be close to 0.5 for balanced dataset)\n",
    "train_class_distribution = np.mean(y_train)\n",
    "val_class_distribution = np.mean(y_val)\n",
    "test_class_distribution = np.mean(y_test)\n",
    "\n",
    "print(f\"Training set class distribution (should be ~0.5): {train_class_distribution:.4f}\")\n",
    "print(f\"Validation set class distribution: {val_class_distribution:.4f}\")\n",
    "print(f\"Test set class distribution: {test_class_distribution:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell was previously incorrectly swapping train/test sets\n",
    "# The correct split is now done in cell 6 with train/val/test (60/20/20)\n",
    "# This cell is kept for reference but should not be executed\n",
    "# If you need to re-run, skip this cell\n",
    "print(\"This cell has been deprecated. Use the split from cell 6 instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify test set class distribution\n",
    "print(f\"Test set class distribution: {np.mean(y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build improved ANN with regularization and batch normalization\n",
    "# Architecture: Input (30 timesteps x 4 features) -> Dense(64) -> Dropout -> Dense(32) -> Dropout -> Dense(1)\n",
    "# Added L2 regularization and dropout to prevent overfitting\n",
    "# Added batch normalization for better training stability\n",
    "\n",
    "model = Sequential([\n",
    "    # Flatten input first (30 timesteps x 4 features = 120 features)\n",
    "    Dense(64, input_shape=(X_train[0].shape[0] * X_train[0].shape[1],), \n",
    "          activation='relu', \n",
    "          kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Reshape input data for flattened input\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model with improved settings\n",
    "# Using binary_crossentropy for binary classification\n",
    "# Adam optimizer with learning rate scheduling via callbacks\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with validation set, early stopping, and learning rate reduction\n",
    "# Early stopping prevents overfitting by monitoring validation loss\n",
    "# Learning rate reduction helps fine-tune the model\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    validation_data=(X_val_flat, y_val),\n",
    "    epochs=50,  # Increased epochs with early stopping\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions for test data\n",
    "y_hat = model.predict(X_test_flat, verbose=0)\n",
    "\n",
    "# Also get predictions for training and validation sets for comparison\n",
    "y_hat_train = model.predict(X_train_flat, verbose=0)\n",
    "y_hat_val = model.predict(X_val_flat, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame for visualization\n",
    "results = pd.DataFrame({\n",
    "    'y_actual': y_test.flatten(), \n",
    "    'y_predicted': y_hat.flatten(),\n",
    "    'y_predicted_binary': (y_hat.flatten() > 0.5).astype(int)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "results[['y_actual', 'y_predicted']].plot(title=\"Model Performance: Predicted vs Actual\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on all datasets\n",
    "# Note: model.evaluate returns [loss, accuracy, precision, recall] based on metrics in compile\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Training set evaluation\n",
    "train_scores = model.evaluate(X_train_flat, y_train, verbose=0)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Loss: {train_scores[0]:.4f}\")\n",
    "print(f\"  Accuracy: {train_scores[1]:.4f} ({train_scores[1]*100:.2f}%)\")\n",
    "print(f\"  Precision: {train_scores[2]:.4f}\")\n",
    "print(f\"  Recall: {train_scores[3]:.4f}\")\n",
    "\n",
    "# Validation set evaluation\n",
    "val_scores = model.evaluate(X_val_flat, y_val, verbose=0)\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Loss: {val_scores[0]:.4f}\")\n",
    "print(f\"  Accuracy: {val_scores[1]:.4f} ({val_scores[1]*100:.2f}%)\")\n",
    "print(f\"  Precision: {val_scores[2]:.4f}\")\n",
    "print(f\"  Recall: {val_scores[3]:.4f}\")\n",
    "\n",
    "# Test set evaluation\n",
    "test_scores = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Loss: {test_scores[0]:.4f}\")\n",
    "print(f\"  Accuracy: {test_scores[1]:.4f} ({test_scores[1]*100:.2f}%)\")\n",
    "print(f\"  Precision: {test_scores[2]:.4f}\")\n",
    "print(f\"  Recall: {test_scores[3]:.4f}\")\n",
    "\n",
    "# Check for overfitting (training accuracy >> validation/test accuracy)\n",
    "if train_scores[1] - val_scores[1] > 0.1:\n",
    "    print(\"\\n⚠️  WARNING: Potential overfitting detected (training accuracy much higher than validation)\")\n",
    "elif train_scores[1] - val_scores[1] < 0.05:\n",
    "    print(\"\\n✓ Model shows good generalization (training and validation accuracy are close)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model representatiion\n",
    "model_str = json.dumps(serialize_keras_object(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'forex_price_predictor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model so it can be accessed later\n",
    "qb.ObjectStore.Save(model_key, model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qb.ObjectStore.ContainsKey(model_key):\n",
    "    model_str = qb.ObjectStore.Read(model_key)\n",
    "    config = json.loads(model_str)['config']\n",
    "    model = Sequential.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDate = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent data for prediction\n",
    "df = qb.History(symbol, testDate - timedelta(40), testDate).loc[symbol]\n",
    "df_change = df[[\"open\", \"high\", \"low\", \"close\"]].pct_change().dropna()\n",
    "\n",
    "# Validate we have enough data\n",
    "if len(df_change) < 30:\n",
    "    print(f\"Warning: Only {len(df_change)} data points available, need at least 30\")\n",
    "\n",
    "# Prepare model input (same format as training)\n",
    "model_input = []\n",
    "for index, row in df_change.tail(30).iterrows():\n",
    "    model_input.append(np.array(row))\n",
    "model_input = np.array([model_input])\n",
    "\n",
    "# Flatten input to match model architecture\n",
    "model_input_flat = model_input.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with confidence score\n",
    "prediction_prob = model.predict(model_input_flat, verbose=0)[0][0]\n",
    "prediction = round(prediction_prob)\n",
    "\n",
    "if prediction == 0:\n",
    "    direction = \"Down\"\n",
    "else:\n",
    "    direction = \"Up\"\n",
    "\n",
    "confidence = abs(prediction_prob - 0.5) * 2  # Convert to 0-1 scale\n",
    "\n",
    "print(f\"Prediction: {direction}\")\n",
    "print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "print(f\"Raw probability: {prediction_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
